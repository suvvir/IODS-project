heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}x
library(MASS)
learning_scaled$cl <- km$cluster
lda.fit <- lda(cl ~ ., data = learning_scaled)
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
plot(lda.fit, col=as.numeric(learning_scaled$cl), dimen=2)
lda.arrows(lda.fit, myscale = 3, col = "#666666")
pairs(learning2014)
library(ggplot2)
library(GGally)
ggpairs(learning2014)
learning2014 <- read.csv("C:/Users/Admin/Documents/GitHub/IODS-project/data/learning2014.csv")
str(learning2014)
library(ggplot2)
library(GGally)
ggpairs(learning2014)
learning <- subset(learning2014, select=-gender)
learning <- subset(learning, select=-Age)
learning_scaled <- scale(learning)
learning_scaled <- as.data.frame(learning_scaled)
summary(learning_scaled)
set.seed(123)
dist_learning <- dist(learning_scaled)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(dist_learning, k)$tot.withinss})
plot(1:k_max, twcss, type='b')
km <-kmeans(dist_learning, centers = 3)
pairs(learning_scaled, col = km$cluster)
library(MASS)
learning_scaled$cl <- km$cluster
lda.fit <- lda(cl ~ ., data = learning_scaled)
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
plot(lda.fit, col=as.numeric(learning_scaled$cl), dimen=2)
lda.arrows(lda.fit, myscale = 3, col = "#666666")
library(MASS)
learning_scaled$cl <- km$cluster
lda.fit <- lda(cl ~ ., data = learning_scaled)
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
summary(lda.fit)
plot(lda.fit, col=as.numeric(learning_scaled$cl), dimen=2)
lda.arrows(lda.fit, myscale = 3, col = "#666666")
summary(lda.fit)
lda.fit
library(MASS)
learning_scaled$cl <- km$cluster
lda.fit <- lda(cl ~ ., data = learning_scaled)
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
lda.fit
plot(lda.fit, col=as.numeric(learning_scaled$cl), dimen=2)
lda.arrows(lda.fit, myscale = 3, col = "#666666")
dim(learning2014)
setwd("~/yhis-kouluhommat/IODS-project")
library(ggplot2)
library(dplyr)
library(GGally)
data <- read.csv("learning2014.csv", header = T)
keep_columns <- c("gender","Age","attitude", "deep", "stra", "surf", "Points")
learning2014 <- select(data, one_of(keep_columns))
dim(learning2014)
str(learning2014)
p <- ggpairs(learning2014, mapping = aes(col=gender, alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
setwd("~/yhis-kouluhommat/IODS-project")
library(ggplot2)
library(dplyr)
library(GGally)
data <- read.csv("learning2014.csv", header = T)
keep_columns <- c("gender","Age","attitude", "deep", "stra", "surf", "Points")
learning2014 <- select(data, one_of(keep_columns))
dim(learning2014)
str(learning2014)
p <- ggpairs(learning2014, mapping = aes(col=gender, alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
p <- ggpairs(learning2014, mapping = aes(col=gender, alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
p <- ggpairs(learning2014, mapping = aes(col=gender, alpha=0.1), lower = list(combo = wrap("facethist", bins = 20)))
p
my_model <- lm(Points ~ attitude + stra + gender, data = learning2014)
summary(my_model)
my_model <- lm(Points ~ attitude + stra + Age, data = learning2014)
summary(my_model)
my_model2 <- lm(Points ~ attitude, data = learning2014)
summary(my_model2)
par(mfrow = c(2,2))
plot(my_model2, which=c(1,2,5))
knitr::opts_chunk$set(echo = TRUE)
setwd("~/yhis-kouluhommat/IODS-project")
library(ggplot2)
library(dplyr)
library(GGally)
library(MASS)
data("Boston")
data(Boston)
data(Boston)
data(Boston)
str(Boston)
summary(Boston)
gather(data) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
setwd("~/yhis-kouluhommat/IODS-project")
library(ggplot2)
library(dplyr)
library(GGally)
library(MASS)
library(tidyr)
library(boot)
gather(data) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits=2)
# print the correlation matrix
cor_matrix
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d",tl.cex = 0.6)
setwd("~/yhis-kouluhommat/IODS-project")
library(ggplot2)
library(dplyr)
library(GGally)
library(MASS)
library(tidyr)
library(boot)
library(corrplot)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits=2)
# print the correlation matrix
cor_matrix
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d",tl.cex = 0.6)
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
summary(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# summary of the scaled crime rate
summary(boston_scaled$crim)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=labels)
# class of the boston_scaled object
class(boston_scaled)
# summary of the scaled crime rate
summary(boston_scaled$crim)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=labels)
# class of the boston_scaled object
class(boston_scaled)
# summary of the scaled crime rate
summary(boston_scaled$crim)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
labels <- c("low","med_low","med_high","high")
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=labels)
# look at the table of the new factor crime
table(crime)
# class of the boston_scaled object
class(boston_scaled)
# summary of the scaled crime rate
summary(boston_scaled$crim)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
labels <- c("low","med_low","med_high","high")
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=labels)
# look at the table of the new factor crime
table(crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# number of rows in the Boston dataset
n <- nrow(boston_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set
test <- boston_scaled[-ind,]
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
# linear discriminant analysis
lda.fit <- lda(crime ~., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 3)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
# center and standardize variables
boston_scale <- scale(Boston)
# euclidean distance matrix
dist_eu <- dist(boston_scale)
# look at the summary of the distances
summary(dist_eu)
# k-means clustering
km <-kmeans(boston_scale, centers = 3)
# plot the Boston dataset with clusters
pairs(boston_scale, col = km$cluster)
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scale, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# k-means clustering
km <-kmeans(boston_scale, centers = 2)
# plot the Boston dataset with clusters
pairs(boston_scale, col = km$cluster)
# k-means clustering
km <-kmeans(boston_scale, centers = 2)
summary(km)
# plot the Boston dataset with clusters
pairs(boston_scale, col = km$cluster)
# k-means clustering
km <-kmeans(boston_scale, centers = 2)
# plot the Boston dataset with clusters
pairs(boston_scale, col = km$cluster)
# k-means clustering
km <-kmeans(boston_scale, centers = 2)
# plot the Boston dataset with clusters
pairs(boston_scale, col = km$cluster)
km$size
# k-means clustering
km <-kmeans(boston_scale, centers = 2)
# plot the Boston dataset with clusters
pairs(boston_scale, col = km$cluster)
km$size
center <-km$centers
center
keep_columns <- c("sex","Medu","famrel", "goout", "high_use")
data <- select(alc, one_of(keep_columns))
setwd("~/yhis-kouluhommat/IODS-project")
library(ggplot2)
library(dplyr::select)
setwd("~/yhis-kouluhommat/IODS-project")
library(dplyr)
library(tidyr)
library(ggplot2)
library(boot)
knitr::opts_chunk$set(echo = TRUE)
keep_columns <- c("sex","Medu","famrel", "goout", "high_use")
data <- select(alc, one_of(keep_columns))
alc <- read.csv("alc.csv", header = T)
str(alc)
keep_columns <- c("sex","Medu","famrel", "goout", "high_use")
data <- select(alc, one_of(keep_columns))
str(data)
keep_columns <- c("sex","Medu","famrel", "goout", "high_use")
data <- select(alc, one_of(keep_columns))
str(data)
setwd("~/yhis-kouluhommat/IODS-project")
library(ggplot2)
library(dplyr)
library(GGally)
library(MASS)
library(tidyr)
library(boot)
library(corrplot)
alc <- read.csv("alc.csv", header = T)
str(alc)
keep_columns <- c("sex","Medu","famrel", "goout", "high_use")
data <- select(alc, one_of(keep_columns))
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
str(hd)
summary(hd)
str(hd)
summary(gii)
colnames(hd)
names(hd) <- c("HDI_rank", "country","HDI_raw","life_expct","educ_expct","mean_educ","GNI_raw","GNI_rank")
colnames(hd)
colnames(gii)
names(gii) <- c("GII_rank", "country","GII_raw","MMR","ABR","parliament","female_educ","male_educ","female_work","male_work")
colnames(gii)
gii <- mutate(gii, FMR_educ = female_educ/male_educ
gii <- mutate(gii, FMR_educ = (female_educ/male_educ)
gii <- mutate(gii, FMR_educ = female_educ/male_educ
gii <- mutate(gii, FMR_work = female_work/male_work)
gii <- mutate(gii, FMR_work = female_work/male_work
gii <- mutate(gii, FMR_educ = female_educ/male_educ
gii <- mutate(gii, FMR_work = female_work/male_work
gii <- mutate(gii, FMR_educ = female_educ/male_educ
# define a new column ratio of Female and Male populations with secondary education
gii <- mutate(gii, FMR_work = female_work/male_work
gii <- mutate(gii, FMR_educ = female_educ/male_educ)
gii <- mutate(gii, FMR_work = female_work/male_work)
# access the libraries
library(dplyr)
library(ggplot2)
gii <- mutate(gii, FMR_work = female_work/male_work)
gii <- mutate(gii, FMR_educ = female_educ/male_educ)
human <- inner_join(gii, hd, by = "country")
write.csv(human, file = "human.csv")
setwd("~/yhis-kouluhommat/IODS-project")
keep_columns <- c("sex","Medu","famrel", "goout", "high_use")
data <- dplyr::select(alc, one_of(keep_columns))
setwd("~/yhis-kouluhommat/IODS-project")
library(ggplot2)
library(dplyr)
library(GGally)
library(MASS)
library(tidyr)
library(boot)
library(corrplot)
alc <- read.csv("alc.csv", header = T)
str(alc)
keep_columns <- c("sex","Medu","famrel", "goout", "high_use")
data <- dplyr::select(alc, one_of(keep_columns))
str(data)
gather(data) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
data %>% group_by(sex, high_use) %>% summarise(count = n(), mean_grade=mean(famrel))
g1 <- ggplot(data, aes(x = high_use, y = famrel, col=sex))
g1 + geom_boxplot() + ylab("family relationships")
data %>% group_by(sex, high_use) %>% summarise(count = n(), mean_grade=mean(goout))
g1 <- ggplot(data, aes(x = high_use, y = goout, col=sex))
g1 + geom_boxplot() + ylab("Going out with friends")
data %>% group_by(sex, high_use) %>% summarise(count = n(), mean_grade=mean(Medu))
g1 <- ggplot(data, aes(x = high_use, y = Medu, col=sex))
g1 + geom_boxplot() + ylab("Mother's education")
m <- glm(high_use ~ sex + famrel + goout + Medu, data = alc, family = "binomial")
summary(m)
OR <- coef(m) %>% exp
CI <- confint(m) %>% exp
cbind(OR, CI)
probabilities <- predict(m, type = "response")
data <- mutate(data, probability = probabilities)
data <- mutate(data, prediction = probability > 0.5)
select(data, famrel, goout, sex, high_use, probability, prediction) %>% tail(10)
probabilities <- predict(m, type = "response")
data <- mutate(data, probability = probabilities)
data <- mutate(data, prediction = probability > 0.5)
dplyr::select(data, famrel, goout, sex, high_use, probability, prediction) %>% tail(10)
table(high_use = data$high_use, prediction = data$prediction)
g <- ggplot(data, aes(x = probability, y = high_use, col = prediction))
g + geom_point()
table(high_use = data$high_use, prediction = data$prediction) %>% prop.table() %>% addmargins()
loss_func <- function(class, prob) {
n_wrong <- abs(class - prob) > 0.5
mean(n_wrong)
}
loss_func(class = data$high_use, prob = data$probability)
cv <- cv.glm(data = data, cost = loss_func, glmfit = m, K = 10)
cv$delta[1]
knitr::opts_chunk$set(echo = TRUE)
setwd("~/yhis-kouluhommat/IODS-project")
library(ggplot2)
library(dplyr)
library(GGally)
library(MASS)
library(tidyr)
library(boot)
library(corrplot)
data(Boston)
str(Boston)
summary(Boston)
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits=2)
# print the correlation matrix
cor_matrix
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos = "b", tl.pos = "d",tl.cex = 0.6)
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
summary(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
# summary of the scaled crime rate
summary(boston_scaled$crim)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
labels <- c("low","med_low","med_high","high")
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=labels)
# look at the table of the new factor crime
table(crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# number of rows in the Boston dataset
n <- nrow(boston_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set
test <- boston_scaled[-ind,]
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
# linear discriminant analysis
lda.fit <- lda(crime ~., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 3)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
# center and standardize variables
boston_scale <- scale(Boston)
# euclidean distance matrix
dist_eu <- dist(boston_scale)
# look at the summary of the distances
summary(dist_eu)
# k-means clustering
km <-kmeans(boston_scale, centers = 3)
# plot the Boston dataset with clusters
pairs(boston_scale, col = km$cluster)
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scale, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# k-means clustering
km <-kmeans(boston_scale, centers = 2)
# plot the Boston dataset with clusters
pairs(boston_scale, col = km$cluster)
km$size
center <-km$centers
center
